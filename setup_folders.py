"""Setup local folder structure for datasets and LLM models."""
import os
import json
from pathlib import Path
from datetime import datetime


def setup_local_storage():
    """Create organized folder structure for ModelOps data."""
    
    # Get project root (absolute path)
    project_root = Path(__file__).parent.absolute()
    
    # Define folder structure
    folders = {
        "data_root": project_root / "modelops_data",
        "datasets": project_root / "modelops_data" / "datasets",
        "datasets_raw": project_root / "modelops_data" / "datasets" / "raw",
        "datasets_processed": project_root / "modelops_data" / "datasets" / "processed",
        "datasets_train": project_root / "modelops_data" / "datasets" / "train",
        "datasets_validation": project_root / "modelops_data" / "datasets" / "validation",
        "datasets_test": project_root / "modelops_data" / "datasets" / "test",
        
        "models": project_root / "modelops_data" / "models",
        "models_base": project_root / "modelops_data" / "models" / "base_models",
        "models_finetuned": project_root / "modelops_data" / "models" / "finetuned",
        "models_checkpoints": project_root / "modelops_data" / "models" / "checkpoints",
        "models_adapters": project_root / "modelops_data" / "models" / "adapters",
        
        "artifacts": project_root / "modelops_data" / "artifacts",
        "artifacts_logs": project_root / "modelops_data" / "artifacts" / "logs",
        "artifacts_metrics": project_root / "modelops_data" / "artifacts" / "metrics",
        "artifacts_exports": project_root / "modelops_data" / "artifacts" / "exports",
        
        "cache": project_root / "modelops_data" / "cache",
        "temp": project_root / "modelops_data" / "temp",
    }
    
    print("üöÄ Setting up ModelOps local storage...")
    print(f"üìÅ Root directory: {folders['data_root']}\n")
    
    # Create all folders
    for name, path in folders.items():
        path.mkdir(parents=True, exist_ok=True)
        print(f"‚úÖ Created: {path}")
    
    # Create .gitkeep files to track empty folders
    for name, path in folders.items():
        gitkeep = path / ".gitkeep"
        gitkeep.touch(exist_ok=True)
    
    # Create README files with descriptions
    readme_contents = {
        folders["datasets"]: """# Datasets Directory

Store your training datasets here:

- **raw/**: Original, unprocessed datasets
- **processed/**: Cleaned and preprocessed datasets ready for training
- **train/**: Training splits
- **validation/**: Validation splits  
- **test/**: Test splits

Supported formats: .jsonl, .json, .csv, .parquet, .txt
""",
        folders["models"]: """# Models Directory

Store your LLM models here:

- **base_models/**: Downloaded base models from HuggingFace or local
- **finetuned/**: Your fine-tuned models after training
- **checkpoints/**: Training checkpoints for resuming
- **adapters/**: LoRA/QLoRA adapter weights

The system automatically manages model versioning and metadata.
""",
        folders["artifacts"]: """# Artifacts Directory

Training artifacts and outputs:

- **logs/**: Training logs and progress
- **metrics/**: Performance metrics and evaluation results
- **exports/**: Exported models in different formats (GGUF, ONNX, etc.)

All artifacts are linked to their source models in the database.
""",
    }
    
    for path, content in readme_contents.items():
        readme_file = path / "README.md"
        readme_file.write_text(content)
        print(f"üìù Created README: {readme_file}")
    
    # Create configuration file
    config = {
        "setup_date": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "paths": {name: str(path) for name, path in folders.items()},
        "supported_formats": {
            "datasets": [".jsonl", ".json", ".csv", ".parquet", ".txt", ".arrow"],
            "models": [".bin", ".safetensors", ".pt", ".pth", ".gguf"],
        },
        "max_dataset_size_gb": 100,
        "max_model_size_gb": 50,
    }
    
    config_file = folders["data_root"] / "storage_config.json"
    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"\nüìã Configuration saved: {config_file}")
    
    # Update .env file with absolute paths
    env_file = project_root / ".env"
    env_updates = f"""
# Updated Storage Paths (Absolute) - Generated by setup_folders.py
DATA_DIR={folders['data_root']}
DATASET_DIR={folders['datasets']}
MODEL_DIR={folders['models']}
ARTIFACTS_PATH={folders['artifacts']}
CHECKPOINT_DIR={folders['models_checkpoints']}
CACHE_DIR={folders['cache']}
TEMP_DIR={folders['temp']}
"""
    
    # Read existing .env
    if env_file.exists():
        with open(env_file, 'r') as f:
            env_content = f.read()
        
        # Remove old path definitions
        lines = env_content.split('\n')
        filtered_lines = [
            line for line in lines 
            if not any(key in line for key in [
                'DATA_DIR=', 'DATASET_DIR=', 'MODEL_DIR=', 
                'ARTIFACTS_PATH=', 'CHECKPOINT_DIR=', 'CACHE_DIR=', 'TEMP_DIR='
            ])
        ]
        
        # Append new paths
        updated_content = '\n'.join(filtered_lines) + env_updates
        
        with open(env_file, 'w') as f:
            f.write(updated_content)
        
        print(f"‚úÖ Updated .env file with absolute paths")
    
    print("\n" + "="*60)
    print("‚úÖ Setup complete!")
    print("="*60)
    print(f"\nüìä Summary:")
    print(f"  ‚Ä¢ Root: {folders['data_root']}")
    print(f"  ‚Ä¢ Datasets: {folders['datasets']}")
    print(f"  ‚Ä¢ Models: {folders['models']}")
    print(f"  ‚Ä¢ Artifacts: {folders['artifacts']}")
    print(f"\nüí° Next steps:")
    print("  1. Add your datasets to datasets/raw/")
    print("  2. Add base models to models/base_models/ (or download via UI)")
    print("  3. Start the API: python start_api.py")
    print("  4. Start the frontend: cd frontend && npm run dev")
    print("\nüéØ Ready to fine-tune LLMs!")
    
    return folders


if __name__ == "__main__":
    setup_local_storage()
