{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ ModelOps Fine-Tuning on Google Colab\n",
    "\n",
    "**GPU-Accelerated LLM Fine-Tuning with QLoRA**\n",
    "\n",
    "This notebook replicates your ModelOps platform's fine-tuning workflow, optimized for Google Colab's GPU resources.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ GPU acceleration (T4/A100)\n",
    "- ‚úÖ QLoRA fine-tuning\n",
    "- ‚úÖ Automatic model download\n",
    "- ‚úÖ Local model saving for download\n",
    "- ‚úÖ Compatible with your ModelOps app\n",
    "\n",
    "## Setup\n",
    "1. Connect to GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "2. Run cells in order\n",
    "3. Download your fine-tuned model at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies\n",
    "\n",
    "Install the same dependencies as your ModelOps platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "!pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install ML frameworks\n",
    "!pip install transformers==4.36.0\n",
    "!pip install peft==0.7.0\n",
    "!pip install bitsandbytes==0.41.0\n",
    "!pip install trl==0.7.0\n",
    "!pip install datasets==2.15.0\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install scipy\n",
    "!pip install wandb  # Optional: for logging\n",
    "\n",
    "# Install quantization tools\n",
    "!pip install autoawq==0.1.6\n",
    "\n",
    "# Other utilities\n",
    "!pip install python-dotenv\n",
    "!pip install huggingface-hub\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Setup Hugging Face (Optional)\n",
    "\n",
    "If using gated models, add your Hugging Face token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Optional: Login to Hugging Face for gated models\n",
    "# hf_token = \"your_huggingface_token_here\"\n",
    "# login(hf_token)\n",
    "\n",
    "print(\"Hugging Face setup complete (login if needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Configure your fine-tuning parameters. You can copy these from your ModelOps app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning configuration\n",
    "config = {\n",
    "    # Model settings\n",
    "    \"base_model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # or your chosen model\n",
    "    \"output_dir\": \"./fine_tuned_model\",\n",
    "    \n",
    "    # Dataset settings\n",
    "    \"dataset_name\": \"timdettmers/openassistant-guanaco\",  # or upload your own\n",
    "    \"text_column\": \"text\",\n",
    "    \n",
    "    # LoRA settings\n",
    "    \"lora_rank\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    \n",
    "    # Training settings\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 50,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 50,\n",
    "    \n",
    "    # Memory optimization\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \n",
    "    # Experiment tracking\n",
    "    \"experiment_name\": \"colab_finetune\"\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Load Dataset\n",
    "\n",
    "Download or upload your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Load from Hugging Face Hub\n",
    "if \"dataset_name\" in config and config[\"dataset_name\"]:\n",
    "    dataset = load_dataset(config[\"dataset_name\"], split=\"train\")\n",
    "    print(f\"‚úÖ Loaded {len(dataset)} examples from {config['dataset_name']}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample data:\")\n",
    "    for i in range(min(3, len(dataset))):\n",
    "        print(f\"Example {i+1}: {dataset[i][config['text_column']][:200]}...\")\n",
    "\n",
    "# Alternative: Upload your own CSV file\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv(list(uploaded.keys())[0])\n",
    "# dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(\"Dataset ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Load Base Model\n",
    "\n",
    "Load the base model with 4-bit quantization for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=config[\"load_in_4bit\"],\n",
    "    bnb_4bit_compute_dtype=getattr(torch, config[\"bnb_4bit_compute_dtype\"]),\n",
    "    bnb_4bit_quant_type=config[\"bnb_4bit_quant_type\"],\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config[\"base_model\"],\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config[\"base_model\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {config['base_model']}\")\n",
    "print(f\"üìä Model size: {model.num_parameters()/1e6:.1f}M parameters\")\n",
    "print(f\"üñ•Ô∏è Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Apply LoRA Configuration\n",
    "\n",
    "Configure the model for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"Applying LoRA configuration...\")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=config[\"lora_rank\"],\n",
    "    lora_alpha=config[\"lora_alpha\"],\n",
    "    target_modules=config[\"target_modules\"],\n",
    "    lora_dropout=config[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úÖ LoRA applied\")\n",
    "print(f\"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.1f}M\")\n",
    "print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "\n",
    "# Print LoRA info\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Prepare Dataset for Training\n",
    "\n",
    "Tokenize and format the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[config[\"text_column\"]],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=config[\"max_seq_length\"],\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Split into train/eval\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"‚úÖ Dataset prepared\")\n",
    "print(f\"üìä Train samples: {len(train_dataset)}\")\n",
    "print(f\"üìä Eval samples: {len(eval_dataset)}\")\n",
    "print(f\"üìä Sequence length: {config['max_seq_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Train the Model\n",
    "\n",
    "Start the QLoRA fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"output_dir\"],\n",
    "    num_train_epochs=config[\"num_epochs\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    logging_steps=config[\"logging_steps\"],\n",
    "    save_steps=config[\"save_steps\"],\n",
    "    evaluation_strategy=config[\"evaluation_strategy\"],\n",
    "    eval_steps=config[\"eval_steps\"],\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for simplicity\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=config[\"text_column\"],\n",
    "    max_seq_length=config[\"max_seq_length\"],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"üöÄ Training started...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(config[\"output_dir\"])\n",
    "print(f\"‚úÖ Training complete! Model saved to {config['output_dir']}\")\n",
    "\n",
    "# Show training results\n",
    "training_results = trainer.state.log_history\n",
    "if training_results:\n",
    "    print(\"\\nüìä Final training metrics:\")\n",
    "    final_log = training_results[-1]\n",
    "    for key, value in final_log.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test the Fine-Tuned Model\n",
    "\n",
    "Generate some sample outputs to verify the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "print(\"Testing fine-tuned model...\")\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=config[\"output_dir\"],\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain quantum computing in simple terms:\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Write a short story about a robot:\",\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Model outputs:\")\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    output = pipe(prompt)[0][\"generated_text\"]\n",
    "    # Remove the prompt from output if it's included\n",
    "    if output.startswith(prompt):\n",
    "        output = output[len(prompt):].strip()\n",
    "    print(f\"ü§ñ Response: {output[:200]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Prepare Model for Download\n",
    "\n",
    "Compress the fine-tuned model for easy download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Preparing model for download...\")\n",
    "\n",
    "# Create a zip file of the fine-tuned model\n",
    "zip_filename = f\"{config['experiment_name']}_model.zip\"\n",
    "shutil.make_archive(config[\"experiment_name\"] + \"_model\", 'zip', config[\"output_dir\"])\n",
    "\n",
    "print(f\"‚úÖ Model compressed: {zip_filename}\")\n",
    "print(f\"üìä File size: {os.path.getsize(zip_filename)/1024/1024:.2f} MB\")\n",
    "\n",
    "# Show download link\n",
    "print(\"\\n‚¨áÔ∏è Click below to download your fine-tuned model:\")\n",
    "files.download(zip_filename)\n",
    "\n",
    "print(\"\\nüìã Instructions:\")\n",
    "print(\"1. Download the zip file above\")\n",
    "print(\"2. Upload it to your ModelOps app\")\n",
    "print(\"3. Import the model for deployment\")\n",
    "\n",
    "print(\"\\nüéâ Fine-tuning complete! Your model is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Optional: Quantize for CPU Deployment\n",
    "\n",
    "Quantize the model to GGUF format for CPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Quantize to GGUF for CPU deployment\n",
    "# This requires additional setup and may take time\n",
    "\n",
    "# from autoawq import AutoAWQForCausalLM\n",
    "# from transformers import AwqConfig\n",
    "\n",
    "# print(\"Quantizing model...\")\n",
    "# quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4}\n",
    "# model.quantize(tokenizer, quant_config=quant_config)\n",
    "# model.save_quantized(config[\"output_dir\"] + \"_quantized\")\n",
    "\n",
    "print(\"Quantization skipped - enable if needed for CPU deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
